{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6358196,"sourceType":"datasetVersion","datasetId":3579787},{"sourceId":8171572,"sourceType":"datasetVersion","datasetId":4836275},{"sourceId":8575334,"sourceType":"datasetVersion","datasetId":5127760},{"sourceId":13225568,"sourceType":"datasetVersion","datasetId":8383115},{"sourceId":13233116,"sourceType":"datasetVersion","datasetId":8386819},{"sourceId":13521169,"sourceType":"datasetVersion","datasetId":8585290},{"sourceId":13541284,"sourceType":"datasetVersion","datasetId":8599346}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install scikit-learn\n\nimport os, torch, torchaudio\nprint(\"CUDA available:\", torch.cuda.is_available())\n\n!ls -R /kaggle/input/audeter-subset-3500each-all/audeter_subset_3500each_ALL | head -n 80\n!ls /kaggle/input/ready-to-input-for-training/combined_folder/real | head -n 20\n!ls /kaggle/input/in-the-wild-audio-deepfake/release_in_the_wild/real | head -n 20\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T08:41:01.979257Z","iopub.execute_input":"2025-10-29T08:41:01.979568Z","iopub.status.idle":"2025-10-29T08:44:39.587055Z","shell.execute_reply.started":"2025-10-29T08:41:01.979546Z","shell.execute_reply":"2025-10-29T08:44:39.585932Z"},"_kg_hide-output":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, math, time, json, pathlib, random\nfrom typing import Tuple, Dict\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchaudio\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    confusion_matrix,\n    roc_auc_score,\n    roc_curve,\n    ConfusionMatrixDisplay\n)\nimport matplotlib.pyplot as plt\n\nSEED = 42\nrandom.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\nSAMPLE_RATE = 16000\nDURATION_S  = 1.0   # crop/pad to 1 second\nN_MELS      = 128\nBATCH_SIZE  = 32\nEPOCHS      = 30\n\n# Telephony Aug activation (for baseline Task A = False)\nUSE_AUG = False\n\n# cobination of real datasets from asvspoof2021 and in the wild\nREAL_MAIN_CAP  = 60000   \nREAL_WILD_CAP  = 20000  \n\nFAKE_ROOT       = \"/kaggle/input/audeter-subset-3500each-all/audeter_subset_3500each_ALL\"\nREAL_MAIN_ROOT  = \"/kaggle/input/ready-to-input-for-training/combined_folder/real\"\nREAL_WILD_ROOT  = \"/kaggle/input/in-the-wild-audio-deepfake/release_in_the_wild/real\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T08:44:39.588786Z","iopub.execute_input":"2025-10-29T08:44:39.589076Z","iopub.status.idle":"2025-10-29T08:44:39.613514Z","shell.execute_reply.started":"2025-10-29T08:44:39.589033Z","shell.execute_reply":"2025-10-29T08:44:39.612800Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"dataset (torchaudio-only; no librosa)","metadata":{}},{"cell_type":"code","source":"class TelephonyAug:\n\n    def __init__(self, sample_rate=16000, nb_sr=8000, noise_snr_db=(10, 25)):\n        self.sr = sample_rate\n        self.nb_sr = nb_sr\n        self.noise_snr_db = noise_snr_db\n\n    def _add_noise(self, x: torch.Tensor):\n        snr_db = float(np.random.uniform(*self.noise_snr_db))\n        p_sig = x.pow(2).mean().item() + 1e-12\n        p_noise = p_sig / (10 ** (snr_db / 10))\n        noise = torch.randn_like(x) * math.sqrt(p_noise)\n        return x + noise\n\n    def __call__(self, wav: torch.Tensor):\n        # downsample to narrowband-ish, compand, upsample, bandlimit, add noiseto datasets\n        y = torchaudio.functional.resample(wav, self.sr, self.nb_sr)\n        y = torch.tanh(1.5 * y)\n        y = torchaudio.functional.resample(y, self.nb_sr, self.sr)\n        y = torchaudio.functional.highpass_biquad(y, self.sr, 300)\n        y = torchaudio.functional.lowpass_biquad(y, self.sr, 3400)\n        y = self._add_noise(y)\n        return y\n\n\nclass DeepfakeAudioDatasetTaskA(Dataset):\n    \"\"\"\n    Task A dataset:\n    -Fake: all clips from AUDETER (~80k), label=1\n    -Real: 60k clips from combined_folder/real, label=0\n    -Real: 20k clips from in-the-wild/real, label=0\n    Total: ~80k fake vs ~80k real (balanced datasets).\n    \"\"\"\n    def __init__(\n        self,\n        fake_root: str,\n        real_root_main: str,\n        real_root_wild: str,\n        real_main_cap: int = 60000,\n        real_wild_cap: int = 20000,\n        sample_rate: int = 16000,\n        duration: float = 1.0,\n        n_mels: int = 128,\n        use_aug: bool = False,\n        extensions=(\".wav\", \".flac\", \".mp3\")\n    ):\n        self.sample_rate = sample_rate\n        self.duration = duration\n        self.max_len = int(sample_rate * duration)\n        self.n_mels = n_mels\n        self.use_aug = use_aug\n\n        fake_paths = []\n        if os.path.isdir(fake_root):\n            for root, dirs, files in os.walk(fake_root):\n                for f in files:\n                    if f.lower().endswith(extensions):\n                        fake_paths.append(os.path.join(root, f))\n        fake_labels = [1] * len(fake_paths)\n\n        real_paths_main = []\n        if os.path.isdir(real_root_main):\n            for f in os.listdir(real_root_main):\n                p = os.path.join(real_root_main, f)\n                if os.path.isfile(p) and f.lower().endswith(extensions):\n                    real_paths_main.append(p)\n        random.shuffle(real_paths_main)\n        real_paths_main = real_paths_main[:real_main_cap]\n\n        real_paths_wild = []\n        if os.path.isdir(real_root_wild):\n            for f in os.listdir(real_root_wild):\n                p = os.path.join(real_root_wild, f)\n                if os.path.isfile(p) and f.lower().endswith(extensions):\n                    real_paths_wild.append(p)\n        random.shuffle(real_paths_wild)\n        real_paths_wild = real_paths_wild[:real_wild_cap]\n\n        real_paths = real_paths_main + real_paths_wild\n        real_labels = [0] * len(real_paths)\n\n        self.audio_paths = fake_paths + real_paths\n        self.labels = fake_labels + real_labels\n\n        print(f\"[TaskA Dataset] fake={len(fake_paths)} real={len(real_paths)} total={len(self.audio_paths)}\")\n\n        # Feature transforms: MelSpectrogram + dB\n        self.melspec = torchaudio.transforms.MelSpectrogram(\n            sample_rate=self.sample_rate,\n            n_mels=self.n_mels,\n            n_fft=1024,\n            hop_length=256,\n            power=2.0\n        )\n        self.to_db = torchaudio.transforms.AmplitudeToDB(stype=\"power\")\n\n        self.tel_aug = TelephonyAug(sample_rate=self.sample_rate) if self.use_aug else None\n\n    def __len__(self):\n        return len(self.audio_paths)\n\n    def _load_resample_mono(self, path: str) -> torch.Tensor:\n        wav, sr = torchaudio.load(path)  \n        wav = wav.mean(dim=0)  \n        if sr != self.sample_rate:\n            wav = torchaudio.functional.resample(wav, sr, self.sample_rate)\n\n        # use pad or trim to fixed length\n        if wav.shape[0] < self.max_len:\n            wav = F.pad(wav, (0, self.max_len - wav.shape[0]))\n        else:\n            wav = wav[: self.max_len]\n        return wav\n\n    def __getitem__(self, idx: int):\n        path = self.audio_paths[idx]\n        label = self.labels[idx]\n\n        wav = self._load_resample_mono(path)  \n        if self.tel_aug is not None:\n            wav = self.tel_aug(wav)\n\n        wav_2d = wav.unsqueeze(0)  \n\n        mel = self.melspec(wav_2d)  \n        mel_db = self.to_db(mel)\n\n        return wav_2d.float(), mel_db.float(), torch.tensor(label).long()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T08:44:39.614308Z","iopub.execute_input":"2025-10-29T08:44:39.614532Z","iopub.status.idle":"2025-10-29T08:44:39.766311Z","shell.execute_reply.started":"2025-10-29T08:44:39.614514Z","shell.execute_reply":"2025-10-29T08:44:39.765483Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we:\n\ndefine TelephonyAug,\n\ndefine DeepfakeAudioDatasetTaskA that:\n\nloads all fake from AUDETER,\n\nloads 60k real from combined_folder/real,\n\nloads 20k real from in-the-wild/real,\n\ndoes waveform trimming/padding,\n\nmakes Mel-spectrogram.","metadata":{}},{"cell_type":"code","source":"class SpecRNet(nn.Module):\n\n    # CNN over Mel-spectrogram - 128-D embedding\n    def __init__(self):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n        self.fc = nn.Linear(64, 128)\n\n    def forward(self, x):\n        x = self.features(x)    \n        x = x.view(x.size(0), -1) \n        return self.fc(x)       \n\n\nclass RawGATST(nn.Module):\n\n    # Lightweight 1D conv stack on raw waveform - 128-D embedding\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv1d(1, 64, kernel_size=5, stride=2, padding=2),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),\n            nn.BatchNorm1d(128),\n            nn.ReLU()\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Linear(128, 128)\n\n    def forward(self, x):\n        x = self.conv(x)              \n        x = self.pool(x).squeeze(-1)  \n        return self.fc(x)             \n\n\nclass FusionNet(nn.Module):\n\n    # Fuse raw branch + mel branch \n    def __init__(self):\n        super().__init__()\n        self.spec_model = SpecRNet()\n        self.raw_model  = RawGATST()\n        self.classifier = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 2)\n        )\n\n    def forward(self, raw_wave, mel_spec):\n        raw_feat  = self.raw_model(raw_wave)    \n        spec_feat = self.spec_model(mel_spec)   \n        fused = torch.cat((raw_feat, spec_feat), dim=1)  \n        return self.classifier(fused)           \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-29T08:44:39.767870Z","iopub.execute_input":"2025-10-29T08:44:39.768161Z","iopub.status.idle":"2025-10-29T08:44:39.780732Z","shell.execute_reply.started":"2025-10-29T08:44:39.768133Z","shell.execute_reply":"2025-10-29T08:44:39.780130Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1.\tSpecRNet → CNN applied on Mel-spectrograms (2D input).\n(mel_spec): a time–frequency representation, closer to how humans perceive sound, useful to detect spectral artifacts in fakes\n2.\tRawGAT-ST → CNN applied on raw waveforms (1D input).\n(raw_wave): the direct audio signal, useful to capture fine low-level details (noise, glitches).\n3.\tFeature Fusion → Concatenation of both feature vectors.\n---->processes two views of the same audio.\n","metadata":{}},{"cell_type":"markdown","source":"models (SpecRNet, RawGATST, FusionNet)","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, loader, device):\n    model.eval()\n    total = 0\n    correct = 0\n    total_loss = 0.0\n    loss_fn = nn.CrossEntropyLoss()\n    for raw, mel, labels in loader:\n        raw, mel, labels = raw.to(device), mel.to(device), labels.to(device)\n        outputs = model(raw, mel)\n        loss = loss_fn(outputs, labels)\n        total_loss += loss.item() * labels.size(0)\n        preds = outputs.argmax(1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    return total_loss / max(1, total), correct / max(1, total)\n\n\ndef count_params(model: nn.Module):\n    return sum(p.numel() for p in model.parameters())\n\n\ndef file_size_mb(path: str) -> float:\n    p = pathlib.Path(path)\n    return p.stat().st_size / (1024 * 1024)\n\n\ndef benchmark_inference(model, loader, device=torch.device(\"cpu\")):\n    model = model.to(device).eval()\n    start = time.time()\n    n = 0\n    with torch.no_grad():\n        for raw, mel, _ in loader:\n            raw, mel = raw.to(device), mel.to(device)\n            _ = model(raw, mel)\n            n += raw.size(0)\n    dt = time.time() - start\n    return {\n        \"batches\": len(loader),\n        \"samples\": n,\n        \"total_sec\": dt,\n        \"sec_per_batch\": dt / max(1, len(loader)),\n        \"ms_per_sample\": 1000.0 * dt / max(1, n),\n    }\n\n\ndef eval_confusion_auc_eer(model, loader, device=torch.device(\"cpu\")):\n    model = model.to(device).eval()\n    ys, ps = [], []\n    with torch.no_grad():\n        for raw, mel, y in loader:\n            raw, mel = raw.to(device), mel.to(device)\n            prob_fake = F.softmax(model(raw, mel), dim=1)[:, 1].cpu().numpy()\n            ys.append(y.numpy()); ps.append(prob_fake)\n    y_true = np.concatenate(ys)\n    y_score = np.concatenate(ps)\n\n    # confusion matrix\n    y_pred = (y_score >= 0.5).astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n\n    # roc-auc\n    try:\n        auc = roc_auc_score(y_true, y_score)\n    except ValueError:\n        auc = float(\"nan\")\n\n    # eer\n    fpr, tpr, th = roc_curve(y_true, y_score)\n    fnr = 1 - tpr\n    idx = np.nanargmin(np.abs(fpr - fnr))\n    eer = 0.5 * (fpr[idx] + fnr[idx])\n    eer_thr = th[idx]\n    return {\n        \"confusion_matrix\": cm.tolist(),\n        \"roc_auc\": float(auc),\n        \"eer\": float(eer),\n        \"eer_threshold\": float(eer_thr)\n    }\n\n\nfrom torch.quantization import quantize_dynamic\ndef quantize_linear_layers(model: nn.Module) -> nn.Module:\n    q_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n    return q_model\n\n\ndef export_onnx(model, out_path=\"fusionnet_taskA.onnx\", raw_len=16000, n_mels=128, t_frames=64):\n\n    model_cpu = model.to(\"cpu\").eval()\n    dummy_raw = torch.randn(1, 1, raw_len)\n    dummy_mel = torch.randn(1, 1, n_mels, t_frames)\n    torch.onnx.export(\n        model_cpu, (dummy_raw, dummy_mel), out_path,\n        input_names=[\"raw\", \"mel\"],\n        output_names=[\"logits\"],\n        opset_version=17,\n        dynamic_axes={\n            \"raw\": {2: \"raw_len\"},\n            \"mel\": {2: \"n_mels\", 3: \"time\"},\n            \"logits\": {1: \"num_classes\"},\n        },\n    )\n    print(\"Exported:\", out_path)\n\n\ndef post_training_report(model, val_loader, test_loader):\n    torch.save(model.state_dict(), \"fusionnet_taskA_fp32.pth\")\n    fp32_mb = file_size_mb(\"fusionnet_taskA_fp32.pth\")\n\n    cpu_stats = benchmark_inference(model, val_loader, device=torch.device(\"cpu\"))\n    metrics   = eval_confusion_auc_eer(model, test_loader, device=torch.device(\"cpu\"))\n\n    q_model = quantize_linear_layers(model)\n    torch.save(q_model.state_dict(), \"fusionnet_taskA_int8_linear.pth\")\n    int8_mb = file_size_mb(\"fusionnet_taskA_int8_linear.pth\")\n\n    q_cpu_stats = benchmark_inference(q_model, val_loader, device=torch.device(\"cpu\"))\n    q_metrics   = eval_confusion_auc_eer(q_model, test_loader, device=torch.device(\"cpu\"))\n\n    report = {\n        \"model_params\": int(count_params(model)),\n        \"files\": {\n            \"fp32_path\": \"fusionnet_taskA_fp32.pth\", \"fp32_mb\": round(fp32_mb, 3),\n            \"int8_path\": \"fusionnet_taskA_int8_linear.pth\", \"int8_mb\": round(int8_mb, 3),\n        },\n        \"cpu_benchmark_fp32\": cpu_stats,\n        \"cpu_benchmark_int8\": q_cpu_stats,\n        \"metrics_fp32\": metrics,\n        \"metrics_int8\": q_metrics,\n    }\n    print(json.dumps(report, indent=2))\n    return report\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T09:14:02.283076Z","iopub.execute_input":"2025-10-28T09:14:02.283974Z","iopub.status.idle":"2025-10-28T09:14:02.300465Z","shell.execute_reply.started":"2025-10-28T09:14:02.283950Z","shell.execute_reply":"2025-10-28T09:14:02.299603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"taskA_dataset = DeepfakeAudioDatasetTaskA(\n    fake_root=FAKE_ROOT,\n    real_root_main=REAL_MAIN_ROOT,\n    real_root_wild=REAL_WILD_ROOT,\n    real_main_cap=REAL_MAIN_CAP,\n    real_wild_cap=REAL_WILD_CAP,\n    sample_rate=SAMPLE_RATE,\n    duration=DURATION_S,\n    n_mels=N_MELS,\n    use_aug=USE_AUG\n)\n\n# split 60% train / 20% val / 20% test\nindices = list(range(len(taskA_dataset)))\nlabels_for_strat = [taskA_dataset[i][2].item() for i in indices]\n\ntrain_idx, temp_idx = train_test_split(\n    indices,\n    test_size=0.4,\n    random_state=SEED,\n    shuffle=True,\n    stratify=labels_for_strat\n)\nval_idx, test_idx = train_test_split(\n    temp_idx,\n    test_size=0.5,\n    random_state=SEED,\n    shuffle=True,\n    stratify=[labels_for_strat[i] for i in temp_idx]\n)\n\ntrain_set_A = Subset(taskA_dataset, train_idx)\nval_set_A   = Subset(taskA_dataset, val_idx)\ntest_set_A  = Subset(taskA_dataset, test_idx)\n\ntrain_loader_A = DataLoader(\n    train_set_A, batch_size=BATCH_SIZE, shuffle=True,\n    num_workers=2, pin_memory=True\n)\nval_loader_A = DataLoader(\n    val_set_A, batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=2, pin_memory=True\n)\ntest_loader_A = DataLoader(\n    test_set_A, batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=2, pin_memory=True\n)\n\nprint(\"Task A split sizes:\",\n      len(train_set_A), len(val_set_A), len(test_set_A))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T10:01:38.135255Z","iopub.execute_input":"2025-10-29T10:01:38.136143Z","iopub.status.idle":"2025-10-29T10:50:10.584685Z","shell.execute_reply.started":"2025-10-29T10:01:38.136109Z","shell.execute_reply":"2025-10-29T10:50:10.583906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random as pyrand\n\nclass StrongAug:\n    # Stronger audio augmentation pipeline intended for Task B@telephony robustness.\n    def __init__(self, sr=16000):\n        self.sr = sr\n        self.tel = TelephonyAug(sample_rate=sr)\n\n    def __call__(self, wav):\n        x = wav\n        if pyrand.random() < 0.7:\n            x = self.tel(x)\n        try:\n            if pyrand.random() < 0.5:\n                rate = pyrand.uniform(0.92, 1.08)\n                x, _ = torchaudio.sox_effects.apply_effects_tensor(\n                    x.unsqueeze(0), self.sr,\n                    [[\"speed\", f\"{rate}\"], [\"rate\", str(self.sr)]]\n                )\n                x = x.squeeze(0)\n        except Exception:\n            pass\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T09:52:08.967508Z","iopub.execute_input":"2025-10-28T09:52:08.967737Z","iopub.status.idle":"2025-10-28T09:52:08.984560Z","shell.execute_reply.started":"2025-10-28T09:52:08.967721Z","shell.execute_reply":"2025-10-28T09:52:08.983773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = FusionNet().to(device)\n\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=1e-3,\n    weight_decay=1e-4\n)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode=\"min\",\n    factor=0.5,\n    patience=2\n)\n\nloss_fn = nn.CrossEntropyLoss()\nscaler = torch.amp.GradScaler(\"cuda\", enabled=torch.cuda.is_available())\n\nPATIENCE    = 5\nCLIP_NORM   = 1.0\nbest_val_loss = float(\"inf\")\nbest_path_A = \"fusionnet_taskA_best.pth\"\nno_improve  = 0\n\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    run_loss = 0.0\n    run_correct = 0\n    seen = 0\n\n    for raw, mel, labels in train_loader_A:\n        raw, mel, labels = raw.to(device), mel.to(device), labels.to(device)\n\n        optimizer.zero_grad(set_to_none=True)\n\n        with torch.amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n            out = model(raw, mel)\n            loss = loss_fn(out, labels)\n\n        scaler.scale(loss).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n        scaler.step(optimizer)\n        scaler.update()\n\n        run_loss    += loss.item() * labels.size(0)\n        run_correct += (out.argmax(1) == labels).sum().item()\n        seen        += labels.size(0)\n\n    train_loss = run_loss / seen\n    train_acc  = run_correct / seen\n\n    val_loss, val_acc = evaluate(model, val_loader_A, device)\n    scheduler.step(val_loss)\n\n    print(f\"[TaskA] Epoch {epoch:02d} | \"\n          f\"train {train_loss:.4f}/{train_acc:.4f} | \"\n          f\"val {val_loss:.4f}/{val_acc:.4f}\")\n\n    if val_loss < best_val_loss - 1e-4:\n        best_val_loss = val_loss\n        no_improve = 0\n        torch.save(model.state_dict(), best_path_A)\n    else:\n        no_improve += 1\n        if no_improve >= PATIENCE:\n            print(\"Early stopping on Task A.\")\n            break\n\nprint(\"Best checkpoint saved to:\", best_path_A)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T09:52:08.986508Z","iopub.execute_input":"2025-10-28T09:52:08.986712Z","iopub.status.idle":"2025-10-28T13:19:10.246123Z","shell.execute_reply.started":"2025-10-28T09:52:08.986696Z","shell.execute_reply":"2025-10-28T13:19:10.243848Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training on ~160K audio segments per epoch is heavy on Kaggle. \n-we interrupted long loops (KeyboardInterrupt) once validation accuracy was already very high, then saved that checkpoint as our Task A model","metadata":{}},{"cell_type":"code","source":"model = FusionNet().to(device)\nmodel.load_state_dict(torch.load(\"fusionnet_taskA_best.pth\", map_location=device))\nmodel.eval()\n\ntest_loss_A, test_acc_A = evaluate(model, test_loader_A, device)\nprint(f\"[TaskA TEST] loss={test_loss_A:.4f}  acc={test_acc_A:.4f}\")\n\nmisclassified = []\ncorrect_pred  = []\n\nwith torch.no_grad():\n    for raw_batch, mel_batch, labels_batch in test_loader_A:\n        raw_batch, mel_batch, labels_batch = (\n            raw_batch.to(device),\n            mel_batch.to(device),\n            labels_batch.to(device),\n        )\n        outputs = model(raw_batch, mel_batch)\n        probs = F.softmax(outputs, dim=1)\n        preds = torch.argmax(probs, dim=1)\n\n        for i in range(len(labels_batch)):\n            true_label = labels_batch[i].item()\n            pred_label = preds[i].item()\n            confidence = float(probs[i][pred_label].item())\n            entry = {\n                \"true\": \"FAKE\" if true_label == 1 else \"REAL\",\n                \"pred\": \"FAKE\" if pred_label == 1 else \"REAL\",\n                \"confidence\": confidence\n            }\n            if true_label != pred_label:\n                misclassified.append(entry)\n            else:\n                correct_pred.append(entry)\n\nprint(f\"Total Correct: {len(correct_pred)}\")\nprint(f\"Total Misclassified: {len(misclassified)}\")\n\nprint(\"\\nSample Misclassifications (up to 10):\")\nfor item in misclassified[:10]:\n    print(f\"True={item['true']} | Pred={item['pred']} | Conf={item['confidence']*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T13:21:36.245530Z","iopub.execute_input":"2025-10-28T13:21:36.246462Z","iopub.status.idle":"2025-10-28T13:30:21.321349Z","shell.execute_reply.started":"2025-10-28T13:21:36.246425Z","shell.execute_reply":"2025-10-28T13:30:21.320331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"QUICK_MODE = False\nMAX_BATCHES = 50  #only used if QUICK MODE = TRUE\n\ndef collect_scores_fast(m, loader, device, quick=False, max_batches=50):\n    m = m.to(device).eval()\n    ys, ps = [], []\n    with torch.inference_mode():\n        for bi, (raw, mel, y) in enumerate(loader):\n            raw = raw.to(device, non_blocking=True)\n            mel = mel.to(device, non_blocking=True)\n            logits = m(raw, mel)\n            prob_fake = torch.softmax(logits, dim=1)[:, 1]\n            ys.append(y.cpu().numpy())\n            ps.append(prob_fake.detach().cpu().numpy())\n            if quick and (bi + 1) >= max_batches:\n                break\n    return np.concatenate(ys), np.concatenate(ps)\n\ny_true, y_score = collect_scores_fast(\n    model, test_loader_A, device, quick=QUICK_MODE, max_batches=MAX_BATCHES\n)\n\nauc = roc_auc_score(y_true, y_score)\nfpr, tpr, thr_all = roc_curve(y_true, y_score)\nfnr = 1 - tpr\neer_idx = np.nanargmin(np.abs(fpr - fnr))\neer = 0.5 * (fpr[eer_idx] + fnr[eer_idx])\neer_thr = float(thr_all[eer_idx])\n\nprint(f\"AUC = {auc:.4f}\")\nprint(f\"EER = {eer:.4f} at threshold = {eer_thr:.4f}\")\n\n# Plot ROC\nplt.figure()\nplt.plot(fpr, tpr, label=f\"ROC (AUC={auc:.3f})\")\nplt.plot([0,1],[0,1],'--', label=\"Chance\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve (Task A model)\")\nplt.legend()\nplt.show()\n\n# confusion Matrix @ EER threshold\ny_pred = (y_score >= eer_thr).astype(int)\ncm = confusion_matrix(y_true, y_pred, labels=[0,1])\nprint(\"Confusion Matrix @ EER threshold [rows: TRUE (REAL,FAKE), cols: PRED (REAL,FAKE)]\")\nprint(cm)\n\nConfusionMatrixDisplay.from_predictions(\n    y_true, y_pred, labels=[0,1], display_labels=[\"REAL\",\"FAKE\"]\n)\nplt.title(f\"Confusion Matrix @ thr={eer_thr:.2f} (Task A)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T13:31:03.613240Z","iopub.execute_input":"2025-10-28T13:31:03.613910Z","iopub.status.idle":"2025-10-28T13:34:02.449468Z","shell.execute_reply.started":"2025-10-28T13:31:03.613870Z","shell.execute_reply":"2025-10-28T13:34:02.448692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"fusionnet_taskA_best.pth\", map_location=device))\nmodel.eval()\n\nreport_taskA = post_training_report(model, val_loader_A, test_loader_A)\n\nwith open(\"taskA_report.json\", \"w\") as f:\n    json.dump(report_taskA, f, indent=2)\n\n!ls -lh /kaggle/working | sed -n '1,200p'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T13:35:23.276455Z","iopub.execute_input":"2025-10-28T13:35:23.277102Z","iopub.status.idle":"2025-10-28T14:16:52.570711Z","shell.execute_reply.started":"2025-10-28T13:35:23.277071Z","shell.execute_reply":"2025-10-28T14:16:52.569817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collect_scores_fast(m, loader, device, quick=False, max_batches=999999):\n    m = m.to(device).eval()\n    ys, ps = [], []\n    with torch.inference_mode():\n        for bi, (raw, mel, y) in enumerate(loader):\n            raw = raw.to(device, non_blocking=True)\n            mel = mel.to(device, non_blocking=True)\n            p = torch.softmax(m(raw, mel), dim=1)[:, 1]\n            ys.append(y.cpu().numpy()); ps.append(p.detach().cpu().numpy())\n            if quick and (bi+1) >= max_batches: break\n    return np.concatenate(ys), np.concatenate(ps)\n\ndef save_roc_cm_png(ckpt_path, test_loader, tag):\n    m = FusionNet().to(device)\n    m.load_state_dict(torch.load(ckpt_path, map_location=device))\n    m.eval()\n\n    y_true, y_score = collect_scores_fast(m, test_loader, device)\n    auc = roc_auc_score(y_true, y_score)\n    fpr, tpr, thr_all = roc_curve(y_true, y_score)\n    fnr = 1 - tpr\n    idx = np.nanargmin(np.abs(fpr - fnr))\n    eer = 0.5 * (fpr[idx] + fnr[idx])\n    thr = float(thr_all[idx])\n    \n    plt.figure()\n    plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\n    plt.plot([0,1],[0,1],'--', label=\"Chance\")\n    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC — {tag}\")\n    plt.legend()\n    plt.savefig(f\"roc_{tag}.png\", dpi=200, bbox_inches=\"tight\")\n    plt.close()\n\n    y_pred = (y_score >= thr).astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n    disp = ConfusionMatrixDisplay(cm, display_labels=[\"REAL\",\"FAKE\"])\n    disp.plot(values_format=\"d\")\n    plt.title(f\"Confusion Matrix — {tag} @ thr={thr:.2f}\")\n    plt.savefig(f\"cm_{tag}.png\", dpi=200, bbox_inches=\"tight\")\n    plt.close()\n\n    print(f\"[{tag}] AUC={auc:.4f} | EER={eer:.4f} | thr={thr:.4f} -> saved roc_{tag}.png, cm_{tag}.png\")\n\nsave_roc_cm_png(\"fusionnet_taskA_best.pth\", test_loader_A, tag=\"taskA\")\n\n!ls -lh /kaggle/working | egrep \"roc_|cm_\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:18:07.765662Z","iopub.execute_input":"2025-10-28T14:18:07.766680Z","iopub.status.idle":"2025-10-28T14:21:13.365975Z","shell.execute_reply.started":"2025-10-28T14:18:07.766643Z","shell.execute_reply":"2025-10-28T14:21:13.365120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def export_onnx_fixed(model, val_loader, out_path=\"fusionnet_taskA.onnx\", raw_len=16000):\n    model_cpu = model.to(\"cpu\").eval()\n\n    try:\n        _, mel_b, _ = next(iter(val_loader))\n        _, _, n_mels, t_frames = mel_b.shape\n        n_mels = int(n_mels)\n        t_frames = int(t_frames)\n    except StopIteration:\n        n_mels, t_frames = 128, 64\n\n    dummy_raw = torch.randn(1, 1, raw_len)\n    dummy_mel = torch.randn(1, 1, n_mels, t_frames)\n\n    torch.onnx.export(\n        model_cpu,\n        (dummy_raw, dummy_mel),\n        out_path,\n        input_names=[\"raw\",\"mel\"],\n        output_names=[\"logits\"],\n        opset_version=17,\n        do_constant_folding=True\n    )\n    print(f\"Exported ONNX -> {out_path} | mel=({n_mels},{t_frames}) raw={raw_len}\")\n\n\nmodel_cpu = FusionNet().to(\"cpu\")\nmodel_cpu.load_state_dict(torch.load(\"fusionnet_taskA_best.pth\", map_location=\"cpu\"))\nexport_onnx_fixed(model_cpu, val_loader_A, out_path=\"fusionnet_taskA.onnx\", raw_len=16000)\n\n!ls -lh /kaggle/working/fusionnet_taskA.onnx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:21:29.329404Z","iopub.execute_input":"2025-10-28T14:21:29.330259Z","iopub.status.idle":"2025-10-28T14:21:30.934812Z","shell.execute_reply.started":"2025-10-28T14:21:29.330225Z","shell.execute_reply":"2025-10-28T14:21:30.933894Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Start TaskB here:","metadata":{}},{"cell_type":"code","source":"# We reuse the same DeepfakeAudioDatasetTaskA class, but with use_aug=True\n# so TelephonyAug is applied\n\nUSE_AUG_TASKB = True\n\ntaskB_dataset = DeepfakeAudioDatasetTaskA(\n    fake_root=FAKE_ROOT,\n    real_root_main=REAL_MAIN_ROOT,\n    real_root_wild=REAL_WILD_ROOT,\n    real_main_cap=REAL_MAIN_CAP,\n    real_wild_cap=REAL_WILD_CAP,\n    sample_rate=SAMPLE_RATE,\n    duration=DURATION_S,\n    n_mels=N_MELS,\n    use_aug=USE_AUG_TASKB\n)\n\nindices_B = list(range(len(taskB_dataset)))\nlabels_B  = [taskB_dataset.labels[i] for i in indices_B]\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_idx_B, temp_idx_B = train_test_split(\n    indices_B,\n    test_size=0.4,\n    random_state=SEED,\n    shuffle=True,\n    stratify=labels_B\n)\n\nval_idx_B, test_idx_B = train_test_split(\n    temp_idx_B,\n    test_size=0.5,\n    random_state=SEED,\n    shuffle=True,\n    stratify=[labels_B[i] for i in temp_idx_B]\n)\n\nfrom torch.utils.data import Subset, DataLoader\n\ntrain_set_B = Subset(taskB_dataset, train_idx_B)\nval_set_B   = Subset(taskB_dataset, val_idx_B)\ntest_set_B  = Subset(taskB_dataset, test_idx_B)\n\ntrain_loader_B = DataLoader(train_set_B, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\nval_loader_B   = DataLoader(val_set_B,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader_B  = DataLoader(test_set_B,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(\"Task B split sizes:\", len(train_set_B), len(val_set_B), len(test_set_B))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T08:58:02.716038Z","iopub.execute_input":"2025-10-29T08:58:02.716785Z","iopub.status.idle":"2025-10-29T09:05:03.393113Z","shell.execute_reply.started":"2025-10-29T08:58:02.716761Z","shell.execute_reply":"2025-10-29T09:05:03.392129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Task A model\nmodel_B = FusionNet().to(device)\nmodel_B.load_state_dict(torch.load(\"fusionnet_taskA_best.pth\", map_location=device))\nmodel_B.train()\n\nprev_weights = {}\nfor name, p in model_B.named_parameters():\n    prev_weights[name] = p.detach().clone()\n\nprint(\"Loaded Task A weights into model_B and snapshotted prev_weights.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:30:12.389757Z","iopub.execute_input":"2025-10-28T14:30:12.390445Z","iopub.status.idle":"2025-10-28T14:30:12.426317Z","shell.execute_reply.started":"2025-10-28T14:30:12.390422Z","shell.execute_reply":"2025-10-28T14:30:12.425616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# freeze backbone to protect learned audio features\nfor p in model_B.spec_model.parameters():\n    p.requires_grad = False\nfor p in model_B.raw_model.parameters():\n    p.requires_grad = False\n# trainable classifiers\nfor p in model_B.classifier.parameters():\n    p.requires_grad = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:30:23.255918Z","iopub.execute_input":"2025-10-28T14:30:23.256480Z","iopub.status.idle":"2025-10-28T14:30:23.261241Z","shell.execute_reply.started":"2025-10-28T14:30:23.256458Z","shell.execute_reply":"2025-10-28T14:30:23.260565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def continual_train_taskB(\n    model,\n    train_loader_new, #telephony augmented data\n    val_loader_old, #clean data that is trained on Task A\n    prev_weights_dict,\n    device,\n    epochs=8,\n    lr=5e-4,\n    reg_lambda=1e-3,\n    clip_norm=1.0,\n    patience=3,\n    save_path=\"fusionnet_taskB_best.pth\"\n):\n\n\n    opt = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=lr,\n        weight_decay=1e-4\n    )\n    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        opt, mode=\"min\", factor=0.5, patience=1\n    )\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    scaler = torch.amp.GradScaler(\"cuda\", enabled=torch.cuda.is_available())\n\n    best_val_loss = float(\"inf\")\n    no_improve = 0\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        run_loss = 0.0\n        run_correct = 0\n        seen = 0\n\n        for raw, mel, labels in train_loader_new:\n            raw, mel, labels = raw.to(device), mel.to(device), labels.to(device)\n\n            opt.zero_grad(set_to_none=True)\n            with torch.amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n                out = model(raw, mel)\n                base_loss = loss_fn(out, labels)\n\n                # regularization term: don't drift too far from Task A weights\n                reg_loss = 0.0\n                for (name, p) in model.named_parameters():\n                    if p.requires_grad and name in prev_weights_dict:\n                        reg_loss = reg_loss + (p - prev_weights_dict[name].to(device)).pow(2).mean()\n\n                loss = base_loss + reg_lambda * reg_loss\n\n            scaler.scale(loss).backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n            scaler.step(opt)\n            scaler.update()\n\n            run_loss += loss.item() * labels.size(0)\n            run_correct += (out.argmax(1) == labels).sum().item()\n            seen += labels.size(0)\n\n        train_loss = run_loss / seen\n        train_acc  = run_correct / seen\n\n        # validate on OLD domain (Task A val set) to measure forgetting\n        val_loss, val_acc = evaluate(model, val_loader_old, device)\n\n        sched.step(val_loss)\n\n        print(f\"[TaskB] Epoch {epoch:02d} | train {train_loss:.4f}/{train_acc:.4f} \"\n              f\"| old_val {val_loss:.4f}/{val_acc:.4f}\")\n\n        # early stopping on OLD domain val loss -> we try not to destroy Task A\n        if val_loss < best_val_loss - 1e-4:\n            best_val_loss = val_loss\n            no_improve = 0\n            torch.save(model.state_dict(), save_path)\n        else:\n            no_improve += 1\n            if no_improve >= patience:\n                print(\"Early stopping Task B.\")\n                break\n\n    print(\"Best continual checkpoint saved to:\", save_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:30:29.452226Z","iopub.execute_input":"2025-10-28T14:30:29.453136Z","iopub.status.idle":"2025-10-28T14:30:29.464287Z","shell.execute_reply.started":"2025-10-28T14:30:29.453106Z","shell.execute_reply":"2025-10-28T14:30:29.463373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fine-tune model_B on Task B data for continual learning\ncontinual_train_taskB(\n    model=model_B,\n    train_loader_new=train_loader_B,\n    val_loader_old=val_loader_A,        \n    prev_weights_dict=prev_weights,\n    device=device,\n    epochs=8,\n    lr=5e-4,\n    reg_lambda=1e-3,\n    save_path=\"fusionnet_taskB_best.pth\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:30:42.775402Z","iopub.execute_input":"2025-10-28T14:30:42.775712Z","iopub.status.idle":"2025-10-28T16:46:42.385458Z","shell.execute_reply.started":"2025-10-28T14:30:42.775689Z","shell.execute_reply":"2025-10-28T16:46:42.384462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"modelB_eval = FusionNet().to(device)\nmodelB_eval.load_state_dict(torch.load(\"fusionnet_taskB_best.pth\", map_location=device))\nmodelB_eval.eval()\n\nloss_B_new, acc_B_new = evaluate(modelB_eval, test_loader_B, device)\nprint(f\"[TaskB model on TaskB audio] loss={loss_B_new:.4f} acc={acc_B_new:.4f}\")\n\nloss_B_old, acc_B_old = evaluate(modelB_eval, test_loader_A, device)\nprint(f\"[TaskB model on TaskA audio] loss={loss_B_old:.4f} acc={acc_B_old:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:46:42.388821Z","iopub.execute_input":"2025-10-28T16:46:42.389207Z","iopub.status.idle":"2025-10-28T16:56:14.260598Z","shell.execute_reply.started":"2025-10-28T16:46:42.389177Z","shell.execute_reply":"2025-10-28T16:56:14.259682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n\ndef collect_scores_for_loader(model, loader, device):\n    model = model.to(device).eval()\n    ys, ps = [], []\n    with torch.no_grad():\n        for raw, mel, y in loader:\n            raw = raw.to(device, non_blocking=True)\n            mel = mel.to(device, non_blocking=True)\n\n            logits = model(raw, mel)\n            prob_fake = torch.softmax(logits, dim=1)[:, 1]  \n\n            ys.append(y.cpu().numpy())\n            ps.append(prob_fake.detach().cpu().numpy())\n\n    y_true = np.concatenate(ys)\n    y_score = np.concatenate(ps)\n    return y_true, y_score\n\ndef save_taskB_roc_cm(taskB_ckpt_path, test_loader_new, tag=\"taskB\"):\n    # load continual model\n    m = FusionNet().to(device)\n    m.load_state_dict(torch.load(taskB_ckpt_path, map_location=device))\n    m.eval()\n\n    y_true, y_score = collect_scores_for_loader(m, test_loader_new, device)\n\n    auc = roc_auc_score(y_true, y_score)\n\n    fpr, tpr, thr_all = roc_curve(y_true, y_score)\n    fnr = 1 - tpr\n    idx = np.nanargmin(np.abs(fpr - fnr))\n    eer = 0.5 * (fpr[idx] + fnr[idx])\n    thr = float(thr_all[idx])\n\n    print(f\"[{tag}] AUC={auc:.4f} | EER={eer:.4f} | thr={thr:.4f}\")\n\n    # ROC \n    plt.figure()\n    plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\n    plt.plot([0,1],[0,1],'--', label=\"Chance\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(f\"ROC — {tag} (continual model)\")\n    plt.legend()\n    plt.savefig(f\"roc_{tag}.png\", dpi=200, bbox_inches=\"tight\")\n    plt.close()\n\n    # Confusion matrix @ EER threshold\n    y_pred = (y_score >= thr).astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n\n    disp = ConfusionMatrixDisplay(cm, display_labels=[\"REAL\",\"FAKE\"])\n    disp.plot(values_format=\"d\")\n    plt.title(f\"Confusion Matrix — {tag} @ thr={thr:.2f}\")\n    plt.savefig(f\"cm_{tag}.png\", dpi=200, bbox_inches=\"tight\")\n    plt.close()\n\n    print(f\"Saved roc_{tag}.png and cm_{tag}.png\")\n    print(\"Confusion matrix:\\n\", cm)\n\nsave_taskB_roc_cm(\n    taskB_ckpt_path=\"fusionnet_taskB_best.pth\",\n    test_loader_new=test_loader_B,\n    tag=\"taskB\"\n)\n\n!ls -lh /kaggle/working | egrep \"roc_|cm_\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:56:14.263958Z","iopub.execute_input":"2025-10-28T16:56:14.264268Z","iopub.status.idle":"2025-10-28T17:00:44.546618Z","shell.execute_reply.started":"2025-10-28T16:56:14.264240Z","shell.execute_reply":"2025-10-28T17:00:44.545382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchaudio\n\ndef export_taskB_to_onnx(taskB_ckpt_path, val_loader_for_shape, out_path=\"fusionnet_taskB.onnx\", raw_len=16000):\n    model_cpu = FusionNet().to(\"cpu\")\n    model_cpu.load_state_dict(torch.load(taskB_ckpt_path, map_location=\"cpu\"))\n    model_cpu.eval()\n\n    try:\n        _, mel_b, _ = next(iter(val_loader_for_shape))\n        _, _, n_mels, t_frames = mel_b.shape\n        n_mels = int(n_mels)\n        t_frames = int(t_frames)\n    except StopIteration:\n        n_mels, t_frames = 128, 64\n\n    # dummy inputs to trace ONNX\n    dummy_raw = torch.randn(1, 1, raw_len)\n    dummy_mel = torch.randn(1, 1, n_mels, t_frames)\n\n    torch.onnx.export(\n        model_cpu,\n        (dummy_raw, dummy_mel),\n        out_path,\n        input_names=[\"raw\",\"mel\"],\n        output_names=[\"logits\"],\n        opset_version=17,\n        do_constant_folding=True\n    )\n\n    print(f\"Exported ONNX -> {out_path} | mel=({n_mels},{t_frames}) raw={raw_len}\")\n\n#export the Task B model using Task B val loader\nexport_taskB_to_onnx(\n    taskB_ckpt_path=\"fusionnet_taskB_best.pth\",\n    val_loader_for_shape=val_loader_B,\n    out_path=\"fusionnet_taskB.onnx\",\n    raw_len=16000\n)\n\n!ls -lh /kaggle/working/fusionnet_taskB.onnx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:00:44.548257Z","iopub.execute_input":"2025-10-28T17:00:44.548622Z","iopub.status.idle":"2025-10-28T17:00:46.493499Z","shell.execute_reply.started":"2025-10-28T17:00:44.548582Z","shell.execute_reply":"2025-10-28T17:00:46.492499Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TaskA vs TaskB ROC + Confusion Matrices","metadata":{}},{"cell_type":"markdown","source":"TaskA vs TaskB ROC + Confusion Matrices (use Telephony-domain eval set (Task B test_loader_B))","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n@torch.no_grad()\ndef collect_scores(model, loader, device):\n    model = model.to(device).eval()\n    all_labels = []\n    all_scores_fake = []  \n    for raw, mel, y in loader:\n        raw = raw.to(device, non_blocking=True)\n        mel = mel.to(device, non_blocking=True)\n        logits = model(raw, mel)          \n        probs = F.softmax(logits, dim=1)  \n        fake_scores = probs[:, 1].detach().cpu().numpy()  \n        all_scores_fake.append(fake_scores)\n        all_labels.append(y.numpy())\n    all_labels = np.concatenate(all_labels)\n    all_scores_fake = np.concatenate(all_scores_fake)\n    return all_labels, all_scores_fake\n\n#load both models\ntaskA_ckpt = \"/kaggle/input/task-bestpth/fusionnet_taskA_best.pth\" \ntaskB_ckpt = \"/kaggle/input/task-bestpth/fusionnet_taskB_best.pth\"   \n\nmodelA = FusionNet()\nmodelA.load_state_dict(torch.load(taskA_ckpt, map_location=device))\nmodelA.eval()\n\nmodelB = FusionNet()\nmodelB.load_state_dict(torch.load(taskB_ckpt, map_location=device))\nmodelB.eval()\n\n# pick which on to compare on:\n# compare on clean domain\n#loader_eval = test_loader_A\n#title_domain = \"Clean-domain eval set (Task A test_loader_A)\"\n\n# compare on telephony/robust domain\nloader_eval = test_loader_B\ntitle_domain = \"Telephony-domain eval set (Task B test_loader_B)\"\n\ny_true_A, y_score_A = collect_scores(modelA, loader_eval, device)\ny_true_B, y_score_B = collect_scores(modelB, loader_eval, device) # use same eval loader\n\n# roc auc graph\nfpr_A, tpr_A, _ = roc_curve(y_true_A, y_score_A)\nauc_A = roc_auc_score(y_true_A, y_score_A)\n\nfpr_B, tpr_B, _ = roc_curve(y_true_B, y_score_B)\nauc_B = roc_auc_score(y_true_B, y_score_B)\n\nplt.figure(figsize=(6,5))\nplt.plot(fpr_A, tpr_A, label=f\"Task A model (AUC={auc_A:.3f})\")\nplt.plot(fpr_B, tpr_B, label=f\"Task B model (AUC={auc_B:.3f})\")\nplt.plot([0,1],[0,1],'--', color='gray', label=\"Chance\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(f\"ROC — Task A vs Task B\\n{title_domain}\")\nplt.legend(loc=\"lower right\")\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"[ROC summary] On {title_domain}:\")\nprint(f\" Task A AUC = {auc_A:.4f}\")\nprint(f\" Task B AUC = {auc_B:.4f}\")\n\n# confusion matrix\nthr_A = 0.1203\nthr_B = 0.5664\n\ny_pred_A = (y_score_A >= thr_A).astype(int) \ny_pred_B = (y_score_B >= thr_B).astype(int)\n\ncm_A = confusion_matrix(y_true_A, y_pred_A, labels=[0,1])\ncm_B = confusion_matrix(y_true_B, y_pred_B, labels=[0,1])\n\n\nfig, axes = plt.subplots(1, 2, figsize=(10,4))\n\nfor ax, cm, title, thr in [\n    (axes[0], cm_A, f\"Task A @ thr={thr_A:.2f}\", thr_A),\n    (axes[1], cm_B, f\"Task B @ thr={thr_B:.2f}\", thr_B),\n]:\n    im = ax.imshow(cm, cmap=\"viridis\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Predicted label\")\n    ax.set_ylabel(\"True label\")\n    ax.set_xticks([0,1])\n    ax.set_yticks([0,1])\n    ax.set_xticklabels([\"REAL\",\"FAKE\"])\n    ax.set_yticklabels([\"REAL\",\"FAKE\"])\n\n    for (i,j), val in np.ndenumerate(cm):\n        ax.text(j, i, f\"{val}\", ha='center', va='center', color='yellow', fontsize=10)\n\n    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n\nplt.suptitle(f\"Confusion Matrices — Task A vs Task B\\n{title_domain}\")\nplt.tight_layout()\nplt.show()\n\nprint(\"Confusion Matrix Task A (rows=true [REAL,FAKE], cols=pred [REAL,FAKE])\")\nprint(cm_A)\nprint(\"Confusion Matrix Task B (rows=true [REAL,FAKE], cols=pred [REAL,FAKE])\")\nprint(cm_B)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T09:05:21.675959Z","iopub.execute_input":"2025-10-29T09:05:21.676540Z","iopub.status.idle":"2025-10-29T09:15:10.500766Z","shell.execute_reply.started":"2025-10-29T09:05:21.676513Z","shell.execute_reply":"2025-10-29T09:15:10.499798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#TaskA vs TaskB ROC + Confusion Matrices (use Clean-domain eval set (Task A test_loader_A))\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n@torch.no_grad()\ndef collect_scores(model, loader, device):\n    model = model.to(device).eval()\n    all_labels = []\n    all_scores_fake = []  \n    for raw, mel, y in loader:\n        raw = raw.to(device, non_blocking=True)\n        mel = mel.to(device, non_blocking=True)\n        logits = model(raw, mel)          \n        probs = F.softmax(logits, dim=1)  \n        fake_scores = probs[:, 1].detach().cpu().numpy()  \n        all_scores_fake.append(fake_scores)\n        all_labels.append(y.numpy())\n    all_labels = np.concatenate(all_labels)\n    all_scores_fake = np.concatenate(all_scores_fake)\n    return all_labels, all_scores_fake\n\n#load both models\ntaskA_ckpt = \"/kaggle/input/task-bestpth/fusionnet_taskA_best.pth\" \ntaskB_ckpt = \"/kaggle/input/task-bestpth/fusionnet_taskB_best.pth\"   \n\nmodelA = FusionNet()\nmodelA.load_state_dict(torch.load(taskA_ckpt, map_location=device))\nmodelA.eval()\n\nmodelB = FusionNet()\nmodelB.load_state_dict(torch.load(taskB_ckpt, map_location=device))\nmodelB.eval()\n\n# pick which on to compare on:\n# compare on clean domain\nloader_eval = test_loader_A\ntitle_domain = \"Clean-domain eval set (Task A test_loader_A)\"\n\n# compare on telephony/robust domain\n# loader_eval = test_loader_B\n# title_domain = \"Telephony-domain eval set (Task B test_loader_B)\"\n\ny_true_A, y_score_A = collect_scores(modelA, loader_eval, device)\ny_true_B, y_score_B = collect_scores(modelB, loader_eval, device) # use same eval loader\n\n# roc auc graph\nfpr_A, tpr_A, _ = roc_curve(y_true_A, y_score_A)\nauc_A = roc_auc_score(y_true_A, y_score_A)\n\nfpr_B, tpr_B, _ = roc_curve(y_true_B, y_score_B)\nauc_B = roc_auc_score(y_true_B, y_score_B)\n\nplt.figure(figsize=(6,5))\nplt.plot(fpr_A, tpr_A, label=f\"Task A model (AUC={auc_A:.3f})\")\nplt.plot(fpr_B, tpr_B, label=f\"Task B model (AUC={auc_B:.3f})\")\nplt.plot([0,1],[0,1],'--', color='gray', label=\"Chance\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(f\"ROC — Task A vs Task B\\n{title_domain}\")\nplt.legend(loc=\"lower right\")\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"[ROC summary] On {title_domain}:\")\nprint(f\" Task A AUC = {auc_A:.4f}\")\nprint(f\" Task B AUC = {auc_B:.4f}\")\n\n# confusion matrix\nthr_A = 0.1203\nthr_B = 0.5664\n\ny_pred_A = (y_score_A >= thr_A).astype(int) \ny_pred_B = (y_score_B >= thr_B).astype(int)\n\ncm_A = confusion_matrix(y_true_A, y_pred_A, labels=[0,1])\ncm_B = confusion_matrix(y_true_B, y_pred_B, labels=[0,1])\n\n\nfig, axes = plt.subplots(1, 2, figsize=(10,4))\n\nfor ax, cm, title, thr in [\n    (axes[0], cm_A, f\"Task A @ thr={thr_A:.2f}\", thr_A),\n    (axes[1], cm_B, f\"Task B @ thr={thr_B:.2f}\", thr_B),\n]:\n    im = ax.imshow(cm, cmap=\"viridis\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Predicted label\")\n    ax.set_ylabel(\"True label\")\n    ax.set_xticks([0,1])\n    ax.set_yticks([0,1])\n    ax.set_xticklabels([\"REAL\",\"FAKE\"])\n    ax.set_yticklabels([\"REAL\",\"FAKE\"])\n\n    for (i,j), val in np.ndenumerate(cm):\n        ax.text(j, i, f\"{val}\", ha='center', va='center', color='yellow', fontsize=10)\n\n    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n\nplt.suptitle(f\"Confusion Matrices — Task A vs Task B\\n{title_domain}\")\nplt.tight_layout()\nplt.show()\n\nprint(\"Confusion Matrix Task A (rows=true [REAL,FAKE], cols=pred [REAL,FAKE])\")\nprint(cm_A)\nprint(\"Confusion Matrix Task B (rows=true [REAL,FAKE], cols=pred [REAL,FAKE])\")\nprint(cm_B)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T10:50:26.025266Z","iopub.execute_input":"2025-10-29T10:50:26.026010Z","iopub.status.idle":"2025-10-29T10:56:04.199234Z","shell.execute_reply.started":"2025-10-29T10:50:26.025985Z","shell.execute_reply":"2025-10-29T10:56:04.198202Z"}},"outputs":[],"execution_count":null}]}